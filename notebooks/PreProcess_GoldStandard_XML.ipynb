{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "* Annotations: https://drive.google.com/file/d/1IH4dL4OKG7bv57K8DreOeSAfJgkgC4sd/view\n",
    "* Relevance score: http://www.trec-cds.org/qrels-treceval-abstracts.2017.txt\n",
    "* Pubmed XML collection: http://trec-cds.appspot.com/2018.html#documents\n",
    "* 2017 Topics: http://trec-cds.appspot.com/topics2017.xml\n",
    "* Extra Abstracts (ASCO and AACR) extracted from the TREC TXT files **@see** [PreProcess_GoldStandard_TXT](PreProcess_GoldStandard_TXT.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import gzip\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decompress _.tar.gz_ Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress Files\n",
    "def decompress(myPath):\n",
    "    fileNames = [f for f in listdir(myPath) if isfile(join(myPath, f)) and f[-7:] == \".tar.gz\"]\n",
    "    for file in fileNames:\n",
    "        print(\"Extracting from: \", file)\n",
    "        tar = tarfile.open(join(myPath, file), \"r:gz\")\n",
    "        tar.extractall(join(myPath, file[:-7]))\n",
    "        tar.close()\n",
    "        print(\"Done\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Path containing the medline_xml.part[x].tar.gz files (Pubmed XML collection)\n",
    "    pubMedAbstracts = \"Y:\\TREC\\XML-Collection\"\n",
    "\n",
    "    # Decompress files\n",
    "    decompress(pubMedAbstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Information from Pubmed XML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Pubmed Ids from the Gold-Standard CSV file\n",
    "def extractDocIDs(filePath):\n",
    "    \"\"\" Extracts all ids from the gold standard \"\"\"\n",
    "    f = pd.read_csv(filePath)\n",
    "    return set(f['trec_doc_id'])\n",
    "\n",
    "# Get the name of the folders containing xml.gz files\n",
    "def getFolderNames(myPath):\n",
    "    dirNames = [d for d in listdir(myPath) if isdir(join(myPath, d))]\n",
    "    return dirNames\n",
    "\n",
    "# Get the name of the xml.gz files\n",
    "def getGzFileNames(myPath):\n",
    "    fileNames = [f for f in listdir(myPath) if isfile(join(myPath, f)) and f[-7:] == \".xml.gz\"]\n",
    "    return fileNames\n",
    "        \n",
    "# Extract relevant information from the papers inside the XML files that match the gold-standard\n",
    "def extractFeatures(folderPath, docIDsPath, outputPath):\n",
    "    st = time.time()\n",
    "    \n",
    "    # Get Pubmed Ids from the Gold-Standard\n",
    "    ids = extractDocIDs(docIDsPath)\n",
    "    print(\"Nr of PMIDs in the Gold-Standard:\", len(ids))\n",
    "    # Recover the names of each folder containing xml.gz files\n",
    "    folderNames = getFolderNames(folderPath)\n",
    "    \n",
    "    nrExtractedXMLs = 0\n",
    "    \n",
    "    # Create CSV for the output\n",
    "    with open(outputPath, 'w', encoding='utf-8') as extractFile:\n",
    "        wr = csv.writer(extractFile, quoting=csv.QUOTE_ALL, delimiter=\"\\t\")\n",
    "        wr.writerow([\"trec_doc_id\",\"title\",\"abstract\",\"major_mesh\",\"minor_mesh\"])\n",
    "    \n",
    "    # Iterate through the folders with the xml.gz files\n",
    "    for folderName in folderNames:\n",
    "        print(\"Looking into files from folder: \", folderName)\n",
    "        gzFiles = getGzFileNames(join(folderPath, folderName))\n",
    "        for gzFileName in gzFiles:\n",
    "            print(\"Analyzing information from: \", gzFileName)\n",
    "            gzFilePath = join(join(folderPath,folderName), gzFileName)\n",
    "            parser = etree.XMLParser(encoding='utf-8', recover=True)\n",
    "            pubMedArticleSet = etree.parse(gzip.open(gzFilePath, 'rt', encoding='utf-8'), parser=parser).getroot()\n",
    "            for mc in pubMedArticleSet.iterfind('PubmedArticle/MedlineCitation'):\n",
    "                pmid = mc.find(\"PMID\").text\n",
    "                # Verify if the PMID is in the list of IDs from the Gold-Standard\n",
    "                majorMeshTerms = []\n",
    "                minorMeshTerms = []\n",
    "                abstractList = []\n",
    "                if pmid in ids:\n",
    "                    # Remove found pmid from ids set\n",
    "                    ids.remove(pmid)\n",
    "                    \n",
    "                    print(\"Extracting info from the PMID: \", pmid)\n",
    "                    # Get title\n",
    "                    if mc.find(\"Article/ArticleTitle\") is not None:\n",
    "                        title = ''.join(mc.find(\"Article/ArticleTitle\").itertext())\n",
    "                    # Get abstract, including the structured ones\n",
    "                    if mc.find(\"Article/Abstract\") is not None:\n",
    "                        for abstractPiece in mc.findall(\"Article/Abstract/AbstractText\"):\n",
    "                            abstractList.append(''.join(abstractPiece.itertext()))\n",
    "                        abstract = ' '.join(abstractList)\n",
    "                    # Extracting major and minor mesh descriptors\n",
    "                    # Qualifiers - not taking into account major and minor attributes\n",
    "                    for meshTerm in mc.findall(\"MeshHeadingList/MeshHeading\"):\n",
    "                        qualifiers = []\n",
    "                        for qualifier in meshTerm.findall(\"QualifierName\"):\n",
    "                            qualifiers.append(meshTerm.find(\"DescriptorName\").text + \"/\" + qualifier.text)\n",
    "                        if not qualifiers:\n",
    "                            fullMesh = meshTerm.find(\"DescriptorName\").text\n",
    "                            if meshTerm.find(\"DescriptorName\").get(\"MajorTopicYN\") == \"Y\":\n",
    "                                majorMeshTerms.append(fullMesh)\n",
    "                            else:\n",
    "                                minorMeshTerms.append(fullMesh)\n",
    "                        else:\n",
    "                            if meshTerm.find(\"DescriptorName\").get(\"MajorTopicYN\") == \"Y\":\n",
    "                                majorMeshTerms.extend(qualifiers)\n",
    "                            else:\n",
    "                                minorMeshTerms.extend(qualifiers)\n",
    "                    majorMeshList = \";\".join(majorMeshTerms)\n",
    "                    minorMeshList = \";\".join(minorMeshTerms)\n",
    "                    \n",
    "                    # Write the result to CSV\n",
    "                    with open(outputPath, 'a', encoding='utf-8') as extractFile:\n",
    "                        wr = csv.writer(extractFile, quoting=csv.QUOTE_ALL, delimiter=\"\\t\")\n",
    "                        wr.writerow([pmid, title, abstract, majorMeshList, minorMeshList])\n",
    "                \n",
    "                    # Count the number of extracted papers\n",
    "                    nrExtractedXMLs += 1\n",
    "    \n",
    "    print(\"Number of papers with information extracted: \", nrExtractedXMLs)\n",
    "    end = time.time()\n",
    "    print(\"Run time: \", end-st)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path containing the medline_xml.partx folders - they need to be extracted first\n",
    "    pubMedAbstracts = \"Y:\\TREC\\XML-Collection\"\n",
    "    # Path containing the Annotated Gold-Standard File\n",
    "    docIDPath = \"Y:\\TREC\\goldDocsAnnotations.csv\"\n",
    "    # Output file\n",
    "    outputPath = \"relevantAbstractsFromXML.csv\"\n",
    "    \n",
    "    # Extract relevant information from the XML files\n",
    "    extractFeatures(pubMedAbstracts, docIDPath, outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Relevant Pubmed Abstracts Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abstracts = pd.read_csv(\"Y:\\TREC\\\\20180621relevantAbstractsFromXML.csv\", sep='\\t', encoding=\"utf-8\", dtype={'trec_doc_id':object})\n",
    "abstracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Extra Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtAbstracts = pd.read_csv(\"Y:\\TREC\\Results\\TXT-Extraction\\\\20180608-relevantAbstracts.csv\", sep='\\t', header=None, names=[\"trec_doc_id\", \"title\", \"abstract\"], dtype={'trec_doc_id':object})\n",
    "extraAbstracts = txtAbstracts[txtAbstracts['trec_doc_id'].str.contains('ASCO|AACR', regex=True)]\n",
    "extraAbstracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Pubmed with Extra Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullAbstracts = pd.concat([abstracts, extraAbstracts], sort=False)\n",
    "fullAbstracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Gold-Standard Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(\"Y:\\TREC\\goldDocsAnnotations.csv\", sep=',', encoding=\"utf-8\", dtype={'trec_topic_number':object})\n",
    "annotations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Relevance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = pd.read_csv(\"Y:\\TREC\\qrels-treceval-abstracts.2017.txt\", sep=' ', encoding=\"utf-8\", header=None, \n",
    "                        names=[\"trec_topic_number\", \"x\", \"trec_doc_id\", \"relevance_score\"], dtype={'trec_topic_number':object})\n",
    "relevance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Information from 2017 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsColumns = ['trec_topic_number', 'trec_topic_disease', 'trec_topic_age', 'trec_topic_sex', 'trec_topic_other1', 'trec_topic_other2', 'trec_topic_other3']\n",
    "topics = pd.DataFrame(columns=topicsColumns)\n",
    "topicsXML = etree.parse(\"Y:\\TREC\\\\topics2017.xml\")\n",
    "for topic in topicsXML.getroot():\n",
    "    topicNumber = topic.get('number')\n",
    "    disease = topic.find('disease').text\n",
    "    demographic = topic.find('demographic').text.split(' ')\n",
    "    age = demographic[0]\n",
    "    sex = demographic[1]\n",
    "    other = topic.find('other').text.split(',')\n",
    "    other1 = other[0]\n",
    "    other2 = None\n",
    "    other3 = None\n",
    "    if len(other) == 2:\n",
    "        other2 = other[1]\n",
    "    if len(other) > 2:\n",
    "        other3 = other[2]\n",
    "    topics = topics.append(pd.Series([topicNumber, disease, age, sex, other1, other2, other3], index=topicsColumns), ignore_index=True)\n",
    "topics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Relevance Score and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotationsRelevance = annotations.merge(relevance, left_on=['trec_topic_number','trec_doc_id'], right_on=['trec_topic_number','trec_doc_id'], how='left')\n",
    "annotationsRelevance.drop([\"x\"], axis=1, inplace=True)\n",
    "annotationsRelevance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Abstracts with Relevance Score and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedAbstracts = annotationsRelevance.merge(fullAbstracts, left_on=['trec_doc_id'], right_on=['trec_doc_id'], how='left')\n",
    "processedAbstracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add 2017 topics Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedGoldStandard = processedAbstracts.merge(topics, left_on=['trec_topic_number'], right_on=['trec_topic_number'], how='left')\n",
    "processedGoldStandard.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Result into a new _.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = time.strftime(\"%Y%m%d\")\n",
    "processedGoldStandard.to_csv(path_or_buf='Y:\\\\TREC\\\\'+ date + 'processedGoldStandard.csv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
